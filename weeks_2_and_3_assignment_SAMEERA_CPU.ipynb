{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlqVBEfLBOF_"
   },
   "source": [
    "**SOW-MKI49: Neural Information Processing Systems**  \n",
    "*Weeks 2 and 3: Assignment (200 points + 20 bonus points + 1 bonus point for each bug you find and another bonus point if you debug it and before you ask, no, typos unfortunately are not considered bugs - first come, first served)*  \n",
    "Author: Umut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT8BaKk5CpeB"
   },
   "outputs": [],
   "source": [
    "# Group 17: ...\n",
    "# Sameera Sandaruwan - s1014012\n",
    "# Student 2 name, student 2 number: ...\n",
    "# Student 3 name, student 3 number: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do List\n",
    "1. Finish questions answers - (image resizing function)\n",
    "2. Training problem - resize\n",
    "3. TotalVariation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00iIAIv37Del"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "data_directory = 'Data' # Make a directory to store the data and enter it here.\n",
    "                    # We will be using a smaller dataset (LFW) than the one used in the paper (CelebA) for computational resource considerations.\n",
    "                    # Download it from http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz.\n",
    "device = -1 #sameera GPU - 0\n",
    "epochs = 4 #100\n",
    "lambda_ = {'feature': 1., 'pixel': 1., 'total_variation': 1e-5}\n",
    "model_directory = 'Model' # Make a directory to store the models and enter it here. Move Vgg4Layers.npz to the model directory.\n",
    "outsize = (96, 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86zZctPu1K2M"
   },
   "source": [
    "**Packages (10 points)**  \n",
    "In this cell, you will import the required packages.  \n",
    "*Tasks*   \n",
    "- (1) It is always good practice to first think about the big picture and not rush into writing code before clearly knowing everything that you will have to do so as to avoid future complications. Therefore, your first task is to study the skeleton code and come up with a plan of how to proceed. (**0 points**)\n",
    "- (2) However, I agree that doing so is arguably the most boring part of coding, and you rather skip it. To help you to resist the temptation of skipping going through the skeleton code, I have removed the import statements. Your second task is to Identify the required packages and import them. Note that if you are using Python 2.7, you should import print from the future. (**10 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBiJw5pV030o"
   },
   "outputs": [],
   "source": [
    "# (2) start\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, serializers, dataset\n",
    "from chainer import Link, Chain, ChainList\n",
    "from chainer.optimizers import Adam\n",
    "from chainer.dataset import concat_examples\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOhjvsOx9lJY"
   },
   "source": [
    "**Preprocessing functions (10 points + 5 bonus points)** (taken from https://github.com/mbeyeler/opencv-python-blueprints)  \n",
    "In the following cell, you will implement some of the preprocessing functions. The rest of the preprocessing steps have already been applied to the data.  \n",
    "*Tasks*\n",
    "- (1) Implement the resizing operation. That is, you should extract the data, resize each portrait to 96 pixels x 96 pixels and save them to the data directory as JPG. (**10 points **)\n",
    "- (2) The pencil sketch class implements the sketch effect in a simpler way than the one mentioned in the lecture. Explain how/why the used operations (blur and divide) convert portraits to sketches, and how it differs from that which was mentioned in the lecture? (**5 bonus points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY4lbpLK9kp4"
   },
   "outputs": [],
   "source": [
    "# (1) start\n",
    "# . Get from Jitendra\n",
    "# .\n",
    "# .\n",
    "# (1) end\n",
    "\n",
    "class PencilSketch:\n",
    "    \"\"\"Pencil sketch effect\n",
    "        A class that applies a pencil sketch effect to an image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width, height, bg_gray='pencilsketch_bg.jpg'): #ERROR: can't have () here --> def __init__(self, (width, height)):\n",
    "        \"\"\"Initialize parameters\n",
    "            :param (width, height): Image size.\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        ### ERROR: useless\n",
    "        # try to open background canvas (if it exists)\n",
    "        self.canvas = cv2.imread(bg_gray, cv2.CV_8UC1)\n",
    "        if self.canvas is not None:\n",
    "            self.canvas = cv2.resize(self.canvas, (self.width, self.height))\n",
    "\n",
    "    def render(self, img_rgb):\n",
    "        \"\"\"Applies pencil sketch effect to an RGB image\n",
    "            :param img_rgb: RGB image to be processed\n",
    "            :returns: Processed RGB image\n",
    "        \"\"\"\n",
    "        img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        img_blur = cv2.GaussianBlur(img_gray, (21, 21), 0, 0)\n",
    "        img_blend = cv2.divide(img_gray, img_blur, scale=256)\n",
    "\n",
    "        # return cv2.cvtColor(img_blend, cv2.COLOR_GRAY2RGB)\n",
    "        return img_blend\n",
    "\n",
    "def pencil_sketch(img_rgb):\n",
    "    pencilSketch = PencilSketch(img_rgb.shape[1], img_rgb.shape[0])\n",
    "\n",
    "    return pencilSketch.render(img_rgb)\n",
    "\n",
    "# (2) Write your answer here.\n",
    "\n",
    "# chainer conv2d kernal works?\n",
    "\n",
    "#find out what this is\n",
    "def preprocess(img):\n",
    "    if img.mode == 'L':\n",
    "        return np.rollaxis(np.asarray(img, 'float32')[..., None], 2)\n",
    "    else:\n",
    "        return np.rollaxis(np.asarray(img, 'float32')[..., ::-1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovQMUuo_7D2k"
   },
   "source": [
    "**Data class**  \n",
    "The following cell defines the data class. It is used to manage the data (loading, etc.). *You do not have to make any changes to the code.*  \n",
    "*Task*\n",
    "- (1) Study the code and refer to the chainer docuimentation if anything is unclear. You will be expected to write similar code by yourself in the coming practicals. (**0 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OF39paH6wff"
   },
   "outputs": [],
   "source": [
    "class Dataset(dataset.DatasetMixin):\n",
    "    def __init__(self, data_files):\n",
    "        self.data_files = data_files\n",
    "\n",
    "    ## ERROR: indentation problem was here\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        t = np.asarray(Image.open(self.data_files[i]).convert('RGB').resize((96, 96), Image.LANCZOS), 'f').transpose(2, 0, 1)\n",
    "        x = pencil_sketch(np.asarray(Image.open(self.data_files[i]).convert('RGB').resize((96, 96), Image.LANCZOS), 'f'))[None]\n",
    "        return t, x #t-target (realistic img), x-input(sketch img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUjEFdDD6xBq"
   },
   "source": [
    "**Model classes (45 points)**  \n",
    "In the following cell you will implement the model classes.\n",
    "*Tasks*   \n",
    "- (1) Implement the layers of the model by filling in the missing code. (**20 points**)\n",
    "- (2) Reimplement the model as a ChainList instead of a Chain. (**5 points**)\n",
    "- (3) Implement the forward pass of the residual block by filling in the missing code. (**20 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "### https://docs.chainer.org/en/stable/glance.html\n",
    "### https://docs.chainer.org/en/stable/guides/index.html\n",
    "### https://docs.chainer.org/en/stable/reference/index.html\n",
    "- what are in_channels, out_channels\n",
    "- wht Functions and Links\n",
    "- param of Convolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nafY2Wgx6QLt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(Chain):\n",
    "    def __init__(self, in_channels, outsize):\n",
    "        super(Model, self).__init__()\n",
    "        ''' just inicializing different layers (Links [L]) '''\n",
    "        with self.init_scope():\n",
    "            # (1) start\n",
    "            # convolution2D_0 = ...\n",
    "            self.convolution2D_0 = L.Convolution2D(in_channels=in_channels, out_channels=32, \n",
    "                                                   ksize=9, stride=1, pad=4)\n",
    "            # batchNormalization_0 = ...\n",
    "            self.batchNormalization_0 = L.BatchNormalization(32)\n",
    "            # convolution2D_1 = ...\n",
    "            self.convolution2D_1 = L.Convolution2D(in_channels=32, out_channels=64, \n",
    "                                                   ksize=3, stride=2, pad=1)\n",
    "            # batchNormalization_1 = ...\n",
    "            self.batchNormalization_1 = L.BatchNormalization(64)\n",
    "            # convolution2D_2 = ...\n",
    "            self.convolution2D_2 = L.Convolution2D(in_channels=64, out_channels=128, \n",
    "                                                   ksize=3, stride=2, pad=1)\n",
    "            # batchNormalization_2 = ...\n",
    "            self.batchNormalization_2 = L.BatchNormalization(128)\n",
    "        \n",
    "            \n",
    "            # residualBlock_3 = ...\n",
    "            self.residualBlock_3 = ResidualBlock(128, 128)\n",
    "            # residualBlock_4 = ...\n",
    "            self.residualBlock_4 = ResidualBlock(128, 128)\n",
    "            # residualBlock_5 = ...\n",
    "            self.residualBlock_5 = ResidualBlock(128, 128)\n",
    "            # residualBlock_6 = ...\n",
    "            self.residualBlock_6 = ResidualBlock(128, 128)\n",
    "            # residualBlock_7 = ...\n",
    "            self.residualBlock_7 = ResidualBlock(128, 128)\n",
    "            \n",
    "            # deconvolution2D_8 = ...\n",
    "            #self.deconvolution2D_8 = L.Deconvolution2D(in_channels=128, out_channels=64, \n",
    "            #                                           ksize=3, stride=2, pad=1, nobias=True, outsize=outsize)\n",
    "            self.deconvolution2D_8 = L.Deconvolution2D(128, 64, 3, 2, 1, True, outsize=(48, 48))\n",
    "            # batchNormalization_8 = ...\n",
    "            self.batchNormalization_8 = L.BatchNormalization(64)\n",
    "            # (1) end\n",
    "            self.deconvolution2D_9 = L.Deconvolution2D(64, 32, 3, 2, 1, True, outsize=outsize)\n",
    "            self.batchNormalization_9 = L.BatchNormalization(32)\n",
    "            self.convolution2D_10 = L.Convolution2D(32, 3, 9, stride=1, pad=4, nobias = True)\n",
    "            self.batchNormalization_10 = L.BatchNormalization(3)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.outsize = outsize\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "        ''' creating the model using the defined \n",
    "        layers and arranging them sequencially. (Functions [F])'''\n",
    "        h = self.convolution2D_0(x)\n",
    "        h = self.batchNormalization_0(h, finetune=finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h)\n",
    "        h = self.batchNormalization_1(h, finetune=finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_2(h)\n",
    "        h = self.batchNormalization_2(h, finetune=finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.residualBlock_3(h, finetune=finetune)\n",
    "        h = self.residualBlock_4(h, finetune=finetune)\n",
    "        h = self.residualBlock_5(h, finetune=finetune)\n",
    "        h = self.residualBlock_6(h, finetune=finetune)\n",
    "        h = self.residualBlock_7(h, finetune=finetune)\n",
    "        h = self.deconvolution2D_8(h)\n",
    "        h = self.batchNormalization_8(h, finetune=finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.deconvolution2D_9(h)\n",
    "        h = self.batchNormalization_9(h, finetune=finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_10(h)\n",
    "        h = self.batchNormalization_10(h, finetune=finetune)\n",
    "        y = 127.5 * F.tanh(h) + 127.5 # QQQ: what is this\n",
    "\n",
    "        return y\n",
    "\n",
    "# Another way to define a chain is using the ChainList class, \n",
    "# which behaves like a list of links:\n",
    "class ModelCL(ChainList):\n",
    "    # (2) start\n",
    "    def __init__(self, in_channels, outsize):\n",
    "        # super init is the difference in ChainList\n",
    "        super(ModelCL, self).__init__(\n",
    "            L.Convolution2D(in_channels=in_channels, out_channels=32, ksize=9, stride=1, pad=4),#0\n",
    "            L.BatchNormalization(32),#1\n",
    "            L.Convolution2D(in_channels=32, out_channels=64, ksize=3, stride=2, pad=1),#2\n",
    "            L.BatchNormalization(64),#3\n",
    "            L.Convolution2D(in_channels=64, out_channels=128, ksize=3, stride=2, pad=1),#4\n",
    "            L.BatchNormalization(128),#5\n",
    "            ResidualBlock(128, 128),#6\n",
    "            ResidualBlock(128, 128),#7\n",
    "            ResidualBlock(128, 128),#8\n",
    "            ResidualBlock(128, 128),#9\n",
    "            ResidualBlock(128, 128),#10\n",
    "            L.Deconvolution2D(in_channels=128, out_channels=64, ksize=3, stride=2, pad=1, nobias=True, \n",
    "                              outsize=(48,48)),#11\n",
    "            L.BatchNormalization(64),#12\n",
    "            L.Deconvolution2D(64, 32, 3, 2, 1, True, outsize),#13\n",
    "            L.BatchNormalization(32),#14\n",
    "            L.Convolution2D(32, 3, 9, pad = 4, nobias = True),#15\n",
    "            L.BatchNormalization(3)#16\n",
    "        )\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.outsize = outsize\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "        ''' creating the model using the defined \n",
    "        layers and arranging them sequencially. (Functions [F])'''\n",
    "        h = self[0](x)\n",
    "        h = self[1](h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self[2](h)\n",
    "        h = self[3](h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self[4](h)\n",
    "        h = self[5](h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self[6](h, finetune)\n",
    "        h = self[7](h, finetune)\n",
    "        h = self[8](h, finetune)\n",
    "        h = self[9](h, finetune)\n",
    "        h = self[10](h, finetune)\n",
    "        h = self[11](h)\n",
    "        h = self[12](h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self[13](h)\n",
    "        h = self[14](h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self[15](h)\n",
    "        h = self[16](h, finetune)\n",
    "        y = 127.5 * F.tanh(h) + 127.5\n",
    "\n",
    "        return y\n",
    "    # (2) end\n",
    "\n",
    "'''\n",
    "Questions:\n",
    "1. nobias = for neurons no bias variable (b) form (W,b)\n",
    "2. pad ??\n",
    "3. \n",
    "'''    \n",
    "    \n",
    "class ResidualBlock(Chain):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.convolution2D_0 = L.Convolution2D( in_channels, out_channels, ksize=3, \n",
    "                                                   stride=1, pad = 1, nobias = True)\n",
    "            self.batchNormalization_0 = L.BatchNormalization(out_channels)\n",
    "            self.convolution2D_1 = L.Convolution2D(out_channels, out_channels, ksize=3, \n",
    "                                                   stride=1, pad = 1, nobias = True)\n",
    "            self.batchNormalization_1 = L.BatchNormalization(out_channels)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "        # (3) start - configuraiton might be different here\n",
    "        h = self.convolution2D_0(x)\n",
    "        h = self.batchNormalization_0(h, finetune=finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h)\n",
    "        h = self.batchNormalization_1(h, finetune=finetune)\n",
    "        y = h+x # no activation is used; as for the paper\n",
    "        # (3) start\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euDbOQWT1UA8"
   },
   "source": [
    "**Loss classes (45 points)**  \n",
    "In the following cell, you will implement the loss classes.  \n",
    "*Tasks*  \n",
    "- (1) You are provided with a custom VGG-16 implementation. How does it differ than the original implementation? Why can we get away with using the simpler implementation? (**5 points**)\n",
    "- (2) Implement the missing convolution layer of the total variation loss by filling in the missing code. (**10 points**)\n",
    "- (3) Implement the forward pass of the total variation loss by filling in the missing code. (**10 points**)\n",
    "- (4) Implement the feature loss component in the forward pass of the loss function by filling in the missing code. (**10 points**)\n",
    "- (5) Explain why the loss components are scaled. (**5 points**)\n",
    "- (6) Explain why the target features are extracted in test mode. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DqHGhS_1M_x"
   },
   "outputs": [],
   "source": [
    "class Vgg4Layers(Chain):\n",
    "    def __init__(self):\n",
    "        super(Vgg4Layers, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.conv1_1 = L.Convolution2D(  3,  64, 3, pad = 1)\n",
    "            self.conv1_2 = L.Convolution2D( 64,  64, 3, pad = 1)\n",
    "            self.conv2_1 = L.Convolution2D( 64, 128, 3, pad = 1)\n",
    "            self.conv2_2 = L.Convolution2D(128, 128, 3, pad = 1)\n",
    "        #wht is this - adding a persistent value\n",
    "        #Link.add_persistent(self, name='mean', value=np.array([[[[103.939]],[[116.779]],[[ 123.68]]]], dtype='float32'))\n",
    "        #self.mean = np.array([[[[103.939]],[[116.779]],[[ 123.68]]]], dtype='float32')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #broadcast_to = Broadcast a given variable to a given shape.\n",
    "        h = x - F.broadcast_to(self.mean, x.shape)\n",
    "        h = self.conv1_1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv1_2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = self.conv2_1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2_2(h)\n",
    "        y = F.relu(h)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Total Variation Loss\n",
    "# 2. feature loss\n",
    "'''\n",
    "How 2x2 kernal convolution produce the total variation loss\n",
    "how kernal values are initialized\n",
    "how does the 2x2 kernal convolution operation accomplish the total variation loss given in the eqn. \n",
    "Does the 2 layer\n",
    "struct. help with that.\n",
    "\n",
    "Convolve input with the kernel [-1; 1] and square\n",
    "Convolve input with the kernel [-1, 1] and square\n",
    "Sum everything and take the square root\n",
    "\n",
    "'''\n",
    "class TotalVariationLoss(Chain):\n",
    "    def __init__(self):\n",
    "        super(TotalVariationLoss, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.convolution2D_0 = L.Convolution2D(in_channels=3, out_channels=1, ksize=2, nobias=True, \n",
    "                                                   initialW=np.array([3 * [[[-1], [1]]]], 'float32'))\n",
    "            # (2) start\n",
    "            self.convolution2D_1 = L.Convolution2D(in_channels=3, out_channels=1, ksize=2, nobias=True, \n",
    "                                                   initialW=np.array([3 * [[[-1, 1]]]], 'float32'))\n",
    "            # (2) end\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # (3) start\n",
    "        # y = ...\n",
    "        # (3) end\n",
    "        h = self.convolution2D_0(x)\n",
    "        sq1 = np.square(h)\n",
    "        h = self.convolution2D_1(x)\n",
    "        sq2 = np.square(h)\n",
    "        loss_sum = sq1+sq2\n",
    "        y = loss_sum ** 0.5\n",
    "        y = np.sum(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    \n",
    "class LossFunction(object):\n",
    "    def __init__(self, lambda_):\n",
    "        self.totalVariationLoss = TotalVariationLoss()\n",
    "        self.vgg4Layers         = Vgg4Layers()\n",
    "\n",
    "    def __call__(self, t, y):            \n",
    "        with chainer.using_config('train', False):\n",
    "            t_ = self.vgg4Layers(t)\n",
    "        # (4) start\n",
    "        # y_ = ...\n",
    "            y_ = self.vgg4Layers(y)\n",
    "        \n",
    "        '''\n",
    "        class chainer.links.VGG16Layers(pretrained_model='auto')\n",
    "        model = chainer.links.VGG16Layers()\n",
    "        t_hat = model.extract(t, layers = [‘conv2_2’], (96, 96))\n",
    "        y_hat = model.extract(y, layers = [‘conv2_2’], (96, 96))\n",
    "        feature_loss = chainer.functions.mean_squared_error(t_hat, y_hat)\n",
    "        '''\n",
    "        \n",
    "        feature_loss = lambda_['feature'] * F.mean_squared_error(t_ , y_)\n",
    "        # (4) end\n",
    "        pixel_loss = lambda_['pixel'] * F.mean_squared_error(t , y)\n",
    "        total_variation_loss = lambda_['total_variation'] * self.totalVariationLoss(y)\n",
    "        loss = feature_loss + pixel_loss + total_variation_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "# (1) Write your answer here.\n",
    "# (5) Write your answer here.\n",
    "# (6) Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nGcCNEy8p3g"
   },
   "source": [
    "**Initialization (10 points)**  \n",
    "The following cell initializes the loss function, the loss history, the model, the optimizer, the datasets and the iterators. *You do not have to make any changes to the code.*  \n",
    "*Tasks*\n",
    "- (1) Study the code and refer to the chainer docuimentation if anything is unclear. You will be expected to write similar code by yourself in the coming practicals. (**0 points**)  \n",
    "- (2) What are the boolean arguments that are passed to the SerialIterator class? (**5 points**)  \n",
    "- (3) Why is it false for the training iterator but not for other iterators? In other words, what would happen if we were to set it to false for the training iterator and true for the other iterators? (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAa-KI4W-3Mm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13233\n"
     ]
    }
   ],
   "source": [
    "lossFunction = LossFunction(lambda_)\n",
    "serializers.load_npz('{:s}/Vgg4Layers.npz'.format(model_directory), lossFunction.vgg4Layers)\n",
    "'''Question\n",
    "What does mean do??\n",
    "'''\n",
    "lossFunction.vgg4Layers.add_persistent('mean', np.array([[[[103.939]], [[116.779]], [[123.68]]]], 'float32'))\n",
    "\n",
    "loss_history = {'training': [], 'validation': []}\n",
    "model = Model(1, outsize) if device < 0 else Model(1, outsize).to_gpu(device)\n",
    "\n",
    "# Adam parameters\n",
    "# alpha = 0.001/0.0002, beta1= 0.9/0.5, beta2 = 0.999, epsilon = 1/0.1/1e-8\n",
    "\n",
    "\n",
    "optimizer = Adam(alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "optimizer.setup(model)\n",
    "\n",
    "\n",
    "import glob\n",
    "data_file = sorted(glob.glob('{}/*/*.jpg'.format(data_directory)))\n",
    "print(len(data_file))\n",
    "\n",
    "train_len = 0.799\n",
    "training_set = Dataset(data_file[:int(train_len * len(data_file))])\n",
    "validation_set = Dataset(data_file[int(train_len * len(data_file)) : int(.8 * len(data_file))])\n",
    "test_set = Dataset(data_file[int(.8 * len(data_file)) :])\n",
    "\n",
    "\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "validation_iterator = iterators.SerialIterator(validation_set, batch_size, False, False)\n",
    "test_iterator = iterators.SerialIterator(test_set , batch_size, False, False)\n",
    "\n",
    "\n",
    "debug_test_set_1 = Dataset(data_file[-4:])\n",
    "debug_test_set_2 = Dataset(data_file[-4:])\n",
    "debug_test_iterator_1 = iterators.SerialIterator(debug_test_set_1 , batch_size, False, False)\n",
    "debug_test_iterator_2 = iterators.SerialIterator(debug_test_set_2 , batch_size, False, False)\n",
    "# (2) Write your answer here.\n",
    "# repeat=False, shuffle=False\n",
    "\n",
    "# (3) Write your answer here.\n",
    "# during the training process, it's batches are selected from a shuffled dataset which makes the training more \n",
    "# generalized. But if we shuffeld the test and validation data, metrics we use to evaluate the model \n",
    "# (accuracy, loss,..) will have much different values due to the shuffel. Hence, using unshuffeled data during test\n",
    "# validation makes it easier to determine that only training will effect the changes in evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 95, 95)\n",
      "(4, 1, 95, 95)\n",
      "---\n",
      "<class 'numpy.ndarray'>\n",
      "(4, 1, 95, 95)\n",
      "<class 'chainer.variable.Variable'>\n",
      "()\n",
      "variable(597145.4)\n",
      "(4, 1, 95, 95)\n",
      "(4, 1, 95, 95)\n",
      "---\n",
      "<class 'numpy.ndarray'>\n",
      "(4, 1, 95, 95)\n",
      "<class 'chainer.variable.Variable'>\n",
      "()\n",
      "variable(615107.6)\n",
      "(4, 1, 95, 95)\n",
      "(4, 1, 95, 95)\n",
      "---\n",
      "<class 'numpy.ndarray'>\n",
      "(4, 1, 95, 95)\n",
      "<class 'chainer.variable.Variable'>\n",
      "()\n",
      "variable(669783.56)\n",
      "(2, 1, 95, 95)\n",
      "(2, 1, 95, 95)\n",
      "---\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 1, 95, 95)\n",
      "<class 'chainer.variable.Variable'>\n",
      "()\n",
      "variable(630671.7)\n"
     ]
    }
   ],
   "source": [
    "aaa_test_set = Dataset(data_file[int(.999 * len(data_file)) :])\n",
    "aaa_iterator = iterators.SerialIterator(aaa_test_set , batch_size, False, True)\n",
    "for j, batch in enumerate(aaa_iterator):\n",
    "    with chainer.using_config('train', True):\n",
    "        t, x = concat_examples(batch=batch, device=device)\n",
    "        y = model(x)\n",
    "        loss = lossFunction(t, y)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | epoch =0 | time=170.46917128562927 sec | loss=456026.4375\n",
      "validation variable(432475.2)\n",
      "validation variable(420631.3)\n",
      "validation variable(464616.16)\n",
      "validation variable(293343.3)\n",
      "validation | epoch =0 | time=83.09318137168884 sec | loss=402766.4921875\n",
      "epoch:   1 / 005, training loss: 456026.4375, validation loss: 402766.4921875.\n",
      "train | epoch =1 | time=196.923424243927 sec | loss=322215.3671875\n",
      "validation variable(424542.78)\n",
      "validation variable(409688.6)\n",
      "validation variable(458491.03)\n",
      "validation variable(289235.12)\n",
      "validation | epoch =1 | time=69.89036130905151 sec | loss=395489.3828125\n",
      "epoch:   2 / 005, training loss: 322215.3671875, validation loss: 395489.3828125.\n",
      "train | epoch =2 | time=171.86837434768677 sec | loss=294491.90625\n",
      "validation variable(429268.97)\n",
      "validation variable(415110.72)\n",
      "validation variable(459646.62)\n",
      "validation variable(283779.75)\n",
      "validation | epoch =2 | time=67.82609534263611 sec | loss=396951.515625\n",
      "epoch:   3 / 005, training loss: 294491.90625, validation loss: 396951.515625.\n",
      "train | epoch =3 | time=196.47498416900635 sec | loss=261264.83203125\n",
      "validation variable(402219.03)\n",
      "validation variable(381555.56)\n",
      "validation variable(422956.53)\n",
      "validation variable(272027.56)\n",
      "validation | epoch =3 | time=75.38920450210571 sec | loss=369689.671875\n",
      "epoch:   4 / 005, training loss: 261264.83203125, validation loss: 369689.671875.\n",
      "train | epoch =4 | time=163.32553791999817 sec | loss=225730.5234375\n",
      "validation variable(510736.34)\n",
      "validation variable(487929.84)\n",
      "validation variable(516817.38)\n",
      "validation variable(416745.03)\n",
      "validation | epoch =4 | time=69.37901544570923 sec | loss=483057.1484375\n",
      "epoch:   5 / 005, training loss: 225730.5234375, validation loss: 483057.1484375.\n"
     ]
    }
   ],
   "source": [
    "# TEST of Training\n",
    "import time\n",
    "\n",
    "for i in range(epochs):#epochs\n",
    "    epoch = i + 1\n",
    "    loss_history['training'].append(0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for j, batch in enumerate(aaa_iterator):\n",
    "        with chainer.using_config('train', True):\n",
    "            t, x = concat_examples(batch=batch, device=device)\n",
    "            y = model(x)\n",
    "            # (1) start\n",
    "            loss = lossFunction(t, y)\n",
    "            # Calculate the gradients in the network\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "            # Update all the trainable parameters\n",
    "            optimizer.update()\n",
    "            # (1) end\n",
    "\n",
    "        loss_history['training'][-1] += float(loss.data)\n",
    "        #print('train',loss)\n",
    "\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    print('train | epoch ={} | time={} sec | loss={}'.format(i, (time.time() - start_time), \n",
    "                                                             loss_history['training'][-1]) )\n",
    "    \n",
    "    # (2) start\n",
    "    loss_history['validation'].append(0)\n",
    "    start_time = time.time()\n",
    "    for j, batch in enumerate(validation_iterator):\n",
    "        with chainer.using_config('train', False):\n",
    "            t, x = concat_examples(batch=batch, device=device)\n",
    "            y = model(x)\n",
    "            loss = lossFunction(t, y)\n",
    "\n",
    "        # ...\n",
    "        loss_history['validation'][-1] += float(loss.data)\n",
    "        #print('validation',loss)\n",
    "\n",
    "    loss_history['validation'][-1] /= j + 1\n",
    "    print('validation | epoch ={} | time={} sec | loss={}'.format(i, (time.time() - start_time), \n",
    "                                                             loss_history['validation'][-1]) )\n",
    "    \n",
    "    # ...\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {}, validation loss: {}.'.format(epoch, epochs, loss_history['training'][-1], loss_history['validation'][-1]))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    # (3) start\n",
    "    serializers.save_npz('{:s}/model_{:03d}.npz'.format(model_directory, epoch), model)\n",
    "    # (3) end\n",
    "    serializers.save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epoch), optimizer)\n",
    "    \n",
    "    training_set = Dataset(data_file[:int(train_len * len(data_file))])\n",
    "    validation_set = Dataset(data_file[int(train_len * len(data_file)) : int(.8 * len(data_file))])\n",
    "    test_set = Dataset(data_file[int(.8 * len(data_file)) :])\n",
    "\n",
    "    training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "    validation_iterator = iterators.SerialIterator(validation_set, batch_size, False, False)\n",
    "    test_iterator = iterators.SerialIterator(test_set , batch_size, False, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9db9c70a90>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[456026.4375, 322215.3671875, 294491.90625, 261264.83203125, 225730.5234375]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9dcadad6d8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_His = np.load('{:s}/loss_history_{:03d}.npz'.format(model_directory, 5))\n",
    "loss_His = loss_His[loss_His.files[0]]\n",
    "loss_His = loss_His.reshape(1,)[0]\n",
    "print(loss_His['training'])\n",
    "\n",
    "plt.plot(loss_His['training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Model object at 0x7f9dc92c8a20>\n"
     ]
    }
   ],
   "source": [
    "model_ld = Model(1, outsize)\n",
    "serializers.load_npz('{:s}/model_{:03d}.npz'.format(model_directory, 5), model_ld)\n",
    "print(model_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ld = Adam(alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "\n",
    "optimizer_ld.setup(model_ld)\n",
    "serializers.load_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, 5), optimizer_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(415819.6)\n"
     ]
    }
   ],
   "source": [
    "# trained model\n",
    "for j, batch in enumerate(debug_test_iterator_1):\n",
    "    with chainer.using_config('train', False):\n",
    "        t, x = concat_examples(batch=batch, device=device)\n",
    "        y = model(x)\n",
    "        loss = lossFunction(t, y)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(415819.6)\n"
     ]
    }
   ],
   "source": [
    "# loaded model\n",
    "for j, batch in enumerate(debug_test_iterator_2):\n",
    "    with chainer.using_config('train', False):\n",
    "        t, x = concat_examples(batch=batch, device=device)\n",
    "        y = model_ld(x)\n",
    "        loss = lossFunction(t, y)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAKIEbqPFzsc"
   },
   "source": [
    "**Training and validation (20 points)**  \n",
    "In the following cell, you will train and validate your model.\n",
    "*Tasks*   \n",
    "- (1) Implement training loss estimation, backprop and parameter update. (**10 points**)\n",
    "- (2) Implement validation loss history (**5 points**)\n",
    "- (3) Implement model serialization  (**5 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13233\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "lossFunction = LossFunction(lambda_)\n",
    "serializers.load_npz('{:s}/Vgg4Layers.npz'.format(model_directory), lossFunction.vgg4Layers)\n",
    "'''Question\n",
    "What does mean do??\n",
    "'''\n",
    "lossFunction.vgg4Layers.add_persistent('mean', np.array([[[[103.939]], [[116.779]], [[123.68]]]], 'float32'))\n",
    "\n",
    "loss_history = {'training': [], 'validation': []}\n",
    "model = Model(1, outsize) if device < 0 else Model(1, outsize).to_gpu(device)\n",
    "\n",
    "# Adam parameters\n",
    "# alpha = 0.001/0.0002, beta1= 0.9/0.5, beta2 = 0.999, epsilon = 1/0.1/1e-8\n",
    "\n",
    "optimizer = Adam(alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "optimizer.setup(model)\n",
    "\n",
    "\n",
    "import glob\n",
    "data_file = sorted(glob.glob('{}/*/*.jpg'.format(data_directory)))\n",
    "print(len(data_file))\n",
    "\n",
    "\n",
    "'''\n",
    "400 imgs - 4 epochs\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "training_set = Dataset(data_file[:400])\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)\n",
    "print(len(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train= 0  loss= variable(575981.94)\n",
      "train= 1  loss= variable(464670.47)\n",
      "train= 2  loss= variable(303798.44)\n",
      "train= 3  loss= variable(332341.53)\n",
      "train= 4  loss= variable(420179.2)\n",
      "train= 5  loss= variable(335374.3)\n",
      "train= 6  loss= variable(313532.06)\n",
      "train= 7  loss= variable(250224.1)\n",
      "train= 8  loss= variable(292573.7)\n",
      "train= 9  loss= variable(320368.62)\n",
      "train= 10  loss= variable(261855.2)\n",
      "train= 11  loss= variable(335226.12)\n",
      "train= 12  loss= variable(280253.06)\n",
      "train= 13  loss= variable(225414.53)\n",
      "train= 14  loss= variable(233650.47)\n",
      "train= 15  loss= variable(320259.7)\n",
      "train= 16  loss= variable(249024.83)\n",
      "train= 17  loss= variable(200192.77)\n",
      "train= 18  loss= variable(216978.12)\n",
      "train= 19  loss= variable(229886.34)\n",
      "train= 20  loss= variable(235219.44)\n",
      "train= 21  loss= variable(164196.88)\n",
      "train= 22  loss= variable(224568.53)\n",
      "train= 23  loss= variable(207084.72)\n",
      "train= 24  loss= variable(151425.06)\n",
      "train= 25  loss= variable(206041.77)\n",
      "train= 26  loss= variable(186656.98)\n",
      "train= 27  loss= variable(188702.27)\n",
      "train= 28  loss= variable(175654.61)\n",
      "train= 29  loss= variable(148000.84)\n",
      "train= 30  loss= variable(198221.77)\n",
      "train= 31  loss= variable(177552.8)\n",
      "train= 32  loss= variable(163692.36)\n",
      "train= 33  loss= variable(155035.66)\n",
      "train= 34  loss= variable(155139.52)\n",
      "train= 35  loss= variable(174100.55)\n",
      "train= 36  loss= variable(168769.6)\n",
      "train= 37  loss= variable(140102.33)\n",
      "train= 38  loss= variable(163565.16)\n",
      "train= 39  loss= variable(152835.67)\n",
      "train= 40  loss= variable(137523.39)\n",
      "train= 41  loss= variable(146861.)\n",
      "train= 42  loss= variable(167587.4)\n",
      "train= 43  loss= variable(140909.64)\n",
      "train= 44  loss= variable(119984.86)\n",
      "train= 45  loss= variable(159224.89)\n",
      "train= 46  loss= variable(164046.25)\n",
      "train= 47  loss= variable(130501.766)\n",
      "train= 48  loss= variable(139482.02)\n",
      "train= 49  loss= variable(145448.1)\n",
      "train= 50  loss= variable(195640.06)\n",
      "train= 51  loss= variable(162166.73)\n",
      "train= 52  loss= variable(146786.7)\n",
      "train= 53  loss= variable(149814.83)\n",
      "train= 54  loss= variable(140026.62)\n",
      "train= 55  loss= variable(138480.05)\n",
      "train= 56  loss= variable(134644.4)\n",
      "train= 57  loss= variable(135123.83)\n",
      "train= 58  loss= variable(138687.56)\n",
      "train= 59  loss= variable(158725.28)\n",
      "train= 60  loss= variable(154816.73)\n",
      "train= 61  loss= variable(169550.5)\n",
      "train= 62  loss= variable(126728.66)\n",
      "train= 63  loss= variable(140656.1)\n",
      "train= 64  loss= variable(131318.38)\n",
      "train= 65  loss= variable(119673.664)\n",
      "train= 66  loss= variable(130766.65)\n",
      "train= 67  loss= variable(178239.42)\n",
      "train= 68  loss= variable(117236.28)\n",
      "train= 69  loss= variable(140891.36)\n",
      "train= 70  loss= variable(141686.23)\n",
      "train= 71  loss= variable(128702.17)\n",
      "train= 72  loss= variable(146784.11)\n",
      "train= 73  loss= variable(129221.9)\n",
      "train= 74  loss= variable(123533.47)\n",
      "train= 75  loss= variable(129843.83)\n",
      "train= 76  loss= variable(125877.64)\n",
      "train= 77  loss= variable(100062.34)\n",
      "train= 78  loss= variable(137421.34)\n",
      "train= 79  loss= variable(98256.086)\n",
      "train= 80  loss= variable(118712.53)\n",
      "train= 81  loss= variable(98041.8)\n",
      "train= 82  loss= variable(112414.82)\n",
      "train= 83  loss= variable(110239.19)\n",
      "train= 84  loss= variable(133325.8)\n",
      "train= 85  loss= variable(129660.3)\n",
      "train= 86  loss= variable(187052.67)\n",
      "train= 87  loss= variable(122787.69)\n",
      "train= 88  loss= variable(110772.266)\n",
      "train= 89  loss= variable(98533.68)\n",
      "train= 90  loss= variable(126956.6)\n",
      "train= 91  loss= variable(118513.7)\n",
      "train= 92  loss= variable(123099.32)\n",
      "train= 93  loss= variable(173402.2)\n",
      "train= 94  loss= variable(141991.33)\n",
      "train= 95  loss= variable(113379.55)\n",
      "train= 96  loss= variable(153283.27)\n",
      "train= 97  loss= variable(169182.58)\n",
      "train= 98  loss= variable(112303.83)\n",
      "train= 99  loss= variable(117446.18)\n",
      "train | epoch =0 | time=4902.551718950272 sec | loss=178923.835078125\n",
      "epoch:   1 / 004, training loss: 178923.835078125, validation loss: 0.0.\n",
      "train= 0  loss= variable(147105.55)\n",
      "train= 1  loss= variable(91506.93)\n",
      "train= 2  loss= variable(154370.19)\n",
      "train= 3  loss= variable(97299.78)\n",
      "train= 4  loss= variable(110254.164)\n",
      "train= 5  loss= variable(99997.4)\n",
      "train= 6  loss= variable(122262.71)\n",
      "train= 7  loss= variable(106499.61)\n",
      "train= 8  loss= variable(106732.36)\n",
      "train= 9  loss= variable(116603.73)\n",
      "train= 10  loss= variable(126538.086)\n",
      "train= 11  loss= variable(102466.32)\n",
      "train= 12  loss= variable(95678.836)\n",
      "train= 13  loss= variable(101635.56)\n",
      "train= 14  loss= variable(103902.8)\n",
      "train= 15  loss= variable(106748.836)\n",
      "train= 16  loss= variable(107451.84)\n",
      "train= 17  loss= variable(104437.71)\n",
      "train= 18  loss= variable(119847.63)\n",
      "train= 19  loss= variable(105981.83)\n",
      "train= 20  loss= variable(89302.33)\n",
      "train= 21  loss= variable(156355.53)\n",
      "train= 22  loss= variable(96831.125)\n",
      "train= 23  loss= variable(133109.55)\n",
      "train= 24  loss= variable(99683.9)\n",
      "train= 25  loss= variable(115099.055)\n",
      "train= 26  loss= variable(111432.54)\n",
      "train= 27  loss= variable(101564.)\n",
      "train= 28  loss= variable(113850.9)\n",
      "train= 29  loss= variable(122301.58)\n",
      "train= 30  loss= variable(90184.54)\n",
      "train= 31  loss= variable(119565.26)\n",
      "train= 32  loss= variable(122733.945)\n",
      "train= 33  loss= variable(129225.04)\n",
      "train= 34  loss= variable(102577.53)\n",
      "train= 35  loss= variable(95222.39)\n",
      "train= 36  loss= variable(123797.57)\n",
      "train= 37  loss= variable(88830.93)\n",
      "train= 38  loss= variable(69216.08)\n",
      "train= 39  loss= variable(99032.47)\n",
      "train= 40  loss= variable(106085.28)\n",
      "train= 41  loss= variable(92040.93)\n",
      "train= 42  loss= variable(103523.11)\n",
      "train= 43  loss= variable(113865.766)\n",
      "train= 44  loss= variable(95667.99)\n",
      "train= 45  loss= variable(117779.914)\n",
      "train= 46  loss= variable(106550.91)\n",
      "train= 47  loss= variable(94413.44)\n",
      "train= 48  loss= variable(125415.414)\n",
      "train= 49  loss= variable(107109.15)\n",
      "train= 50  loss= variable(78914.96)\n",
      "train= 51  loss= variable(81937.2)\n",
      "train= 52  loss= variable(89289.88)\n",
      "train= 53  loss= variable(89164.734)\n",
      "train= 54  loss= variable(82934.016)\n",
      "train= 55  loss= variable(97423.39)\n",
      "train= 56  loss= variable(96257.48)\n",
      "train= 57  loss= variable(105203.82)\n",
      "train= 58  loss= variable(108779.91)\n",
      "train= 59  loss= variable(143407.58)\n",
      "train= 60  loss= variable(78220.91)\n",
      "train= 61  loss= variable(104981.44)\n",
      "train= 62  loss= variable(114932.33)\n",
      "train= 63  loss= variable(133414.38)\n",
      "train= 64  loss= variable(132238.75)\n",
      "train= 65  loss= variable(86236.33)\n",
      "train= 66  loss= variable(120685.02)\n",
      "train= 67  loss= variable(106625.55)\n",
      "train= 68  loss= variable(73430.9)\n",
      "train= 69  loss= variable(98986.46)\n",
      "train= 70  loss= variable(92194.74)\n",
      "train= 71  loss= variable(86442.7)\n",
      "train= 72  loss= variable(117315.375)\n",
      "train= 73  loss= variable(76507.23)\n",
      "train= 74  loss= variable(131424.5)\n",
      "train= 75  loss= variable(88822.24)\n",
      "train= 76  loss= variable(99998.52)\n",
      "train= 77  loss= variable(106149.56)\n",
      "train= 78  loss= variable(102804.53)\n",
      "train= 79  loss= variable(144527.61)\n",
      "train= 80  loss= variable(80320.14)\n",
      "train= 81  loss= variable(118510.586)\n",
      "train= 82  loss= variable(110558.38)\n",
      "train= 83  loss= variable(87257.59)\n",
      "train= 84  loss= variable(101919.32)\n",
      "train= 85  loss= variable(121125.25)\n",
      "train= 86  loss= variable(91614.516)\n",
      "train= 87  loss= variable(85474.64)\n",
      "train= 88  loss= variable(84616.25)\n",
      "train= 89  loss= variable(89973.66)\n",
      "train= 90  loss= variable(136125.84)\n",
      "train= 91  loss= variable(99845.35)\n",
      "train= 92  loss= variable(91317.86)\n",
      "train= 93  loss= variable(75787.195)\n",
      "train= 94  loss= variable(107508.055)\n",
      "train= 95  loss= variable(107338.74)\n",
      "train= 96  loss= variable(99736.09)\n",
      "train= 97  loss= variable(102184.984)\n",
      "train= 98  loss= variable(86265.555)\n",
      "train= 99  loss= variable(94975.95)\n",
      "train | epoch =1 | time=4804.0413727760315 sec | loss=105113.980234375\n",
      "epoch:   2 / 004, training loss: 105113.980234375, validation loss: 0.0.\n",
      "train= 0  loss= variable(73266.33)\n",
      "train= 1  loss= variable(111586.81)\n",
      "train= 2  loss= variable(90201.4)\n",
      "train= 3  loss= variable(98359.23)\n",
      "train= 4  loss= variable(145116.36)\n",
      "train= 5  loss= variable(83156.68)\n",
      "train= 6  loss= variable(93521.9)\n",
      "train= 7  loss= variable(94013.445)\n",
      "train= 8  loss= variable(95850.27)\n",
      "train= 9  loss= variable(111566.61)\n",
      "train= 10  loss= variable(86305.12)\n",
      "train= 11  loss= variable(93867.8)\n",
      "train= 12  loss= variable(85011.53)\n",
      "train= 13  loss= variable(91338.09)\n",
      "train= 14  loss= variable(78772.25)\n",
      "train= 15  loss= variable(119192.914)\n",
      "train= 16  loss= variable(105131.12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train= 17  loss= variable(96816.39)\n",
      "train= 18  loss= variable(99121.71)\n",
      "train= 19  loss= variable(82390.78)\n",
      "train= 20  loss= variable(79636.62)\n",
      "train= 21  loss= variable(84216.18)\n",
      "train= 22  loss= variable(77677.734)\n",
      "train= 23  loss= variable(89350.22)\n",
      "train= 24  loss= variable(81713.73)\n",
      "train= 25  loss= variable(82464.484)\n",
      "train= 26  loss= variable(96040.03)\n",
      "train= 27  loss= variable(104721.734)\n",
      "train= 28  loss= variable(70859.23)\n",
      "train= 29  loss= variable(88001.98)\n",
      "train= 30  loss= variable(98270.89)\n",
      "train= 31  loss= variable(92041.836)\n",
      "train= 32  loss= variable(78683.16)\n",
      "train= 33  loss= variable(88076.)\n",
      "train= 34  loss= variable(88034.375)\n",
      "train= 35  loss= variable(84840.04)\n",
      "train= 36  loss= variable(112608.195)\n",
      "train= 37  loss= variable(109450.195)\n",
      "train= 38  loss= variable(77365.56)\n",
      "train= 39  loss= variable(91418.734)\n",
      "train= 40  loss= variable(84494.63)\n",
      "train= 41  loss= variable(83030.484)\n",
      "train= 42  loss= variable(103069.234)\n",
      "train= 43  loss= variable(104089.945)\n",
      "train= 44  loss= variable(87287.21)\n",
      "train= 45  loss= variable(67808.95)\n",
      "train= 46  loss= variable(97862.38)\n",
      "train= 47  loss= variable(104332.8)\n",
      "train= 48  loss= variable(75008.266)\n",
      "train= 49  loss= variable(93133.02)\n",
      "train= 50  loss= variable(66142.87)\n",
      "train= 51  loss= variable(88552.13)\n",
      "train= 52  loss= variable(67754.24)\n",
      "train= 53  loss= variable(104554.27)\n",
      "train= 54  loss= variable(96230.65)\n",
      "train= 55  loss= variable(88176.04)\n",
      "train= 56  loss= variable(91129.53)\n",
      "train= 57  loss= variable(77833.49)\n",
      "train= 58  loss= variable(88937.664)\n",
      "train= 59  loss= variable(65374.992)\n",
      "train= 60  loss= variable(62706.246)\n",
      "train= 61  loss= variable(103916.55)\n",
      "train= 62  loss= variable(102944.97)\n",
      "train= 63  loss= variable(64371.516)\n",
      "train= 64  loss= variable(78803.6)\n",
      "train= 65  loss= variable(98201.22)\n",
      "train= 66  loss= variable(96253.07)\n",
      "train= 67  loss= variable(80586.13)\n",
      "train= 68  loss= variable(102677.984)\n",
      "train= 69  loss= variable(83103.95)\n",
      "train= 70  loss= variable(65336.29)\n",
      "train= 71  loss= variable(84870.54)\n",
      "train= 72  loss= variable(97027.98)\n",
      "train= 73  loss= variable(99395.25)\n",
      "train= 74  loss= variable(71014.48)\n",
      "train= 75  loss= variable(67877.44)\n",
      "train= 76  loss= variable(101905.02)\n",
      "train= 77  loss= variable(84775.21)\n",
      "train= 78  loss= variable(94169.734)\n",
      "train= 79  loss= variable(105651.81)\n",
      "train= 80  loss= variable(66651.57)\n",
      "train= 81  loss= variable(84441.37)\n",
      "train= 82  loss= variable(76195.08)\n",
      "train= 83  loss= variable(143854.12)\n",
      "train= 84  loss= variable(86568.32)\n",
      "train= 85  loss= variable(61475.85)\n",
      "train= 86  loss= variable(102094.336)\n",
      "train= 87  loss= variable(83462.234)\n",
      "train= 88  loss= variable(110101.89)\n",
      "train= 89  loss= variable(83997.68)\n",
      "train= 90  loss= variable(88786.61)\n",
      "train= 91  loss= variable(69888.83)\n",
      "train= 92  loss= variable(71424.34)\n",
      "train= 93  loss= variable(98422.41)\n",
      "train= 94  loss= variable(92524.7)\n",
      "train= 95  loss= variable(72999.29)\n",
      "train= 96  loss= variable(127284.5)\n",
      "train= 97  loss= variable(66470.945)\n",
      "train= 98  loss= variable(63976.52)\n",
      "train= 99  loss= variable(116954.29)\n",
      "train | epoch =2 | time=4749.977640390396 sec | loss=89580.243359375\n",
      "epoch:   3 / 004, training loss: 89580.243359375, validation loss: 0.0.\n",
      "train= 0  loss= variable(91850.805)\n",
      "train= 1  loss= variable(92765.79)\n",
      "train= 2  loss= variable(108712.38)\n",
      "train= 3  loss= variable(75662.84)\n",
      "train= 4  loss= variable(77616.04)\n",
      "train= 5  loss= variable(120188.445)\n",
      "train= 6  loss= variable(92071.92)\n",
      "train= 7  loss= variable(85887.734)\n",
      "train= 8  loss= variable(73488.98)\n",
      "train= 9  loss= variable(74559.484)\n",
      "train= 10  loss= variable(88921.91)\n",
      "train= 11  loss= variable(72713.53)\n",
      "train= 12  loss= variable(64778.63)\n",
      "train= 13  loss= variable(68889.73)\n",
      "train= 14  loss= variable(78641.164)\n",
      "train= 15  loss= variable(64065.1)\n",
      "train= 16  loss= variable(98687.27)\n",
      "train= 17  loss= variable(96571.99)\n",
      "train= 18  loss= variable(75451.51)\n",
      "train= 19  loss= variable(72640.52)\n",
      "train= 20  loss= variable(67825.58)\n",
      "train= 21  loss= variable(70706.06)\n",
      "train= 22  loss= variable(86710.11)\n",
      "train= 23  loss= variable(99316.87)\n",
      "train= 24  loss= variable(110852.016)\n",
      "train= 25  loss= variable(67824.66)\n",
      "train= 26  loss= variable(62161.5)\n",
      "train= 27  loss= variable(89454.63)\n",
      "train= 28  loss= variable(78623.91)\n",
      "train= 29  loss= variable(51158.152)\n",
      "train= 30  loss= variable(128006.36)\n",
      "train= 31  loss= variable(61336.81)\n",
      "train= 32  loss= variable(87901.945)\n",
      "train= 33  loss= variable(75317.83)\n",
      "train= 34  loss= variable(62669.152)\n",
      "train= 35  loss= variable(102687.79)\n",
      "train= 36  loss= variable(66370.38)\n",
      "train= 37  loss= variable(72893.99)\n",
      "train= 38  loss= variable(62599.48)\n",
      "train= 39  loss= variable(74873.875)\n",
      "train= 40  loss= variable(78916.49)\n",
      "train= 41  loss= variable(90263.72)\n",
      "train= 42  loss= variable(72160.484)\n",
      "train= 43  loss= variable(74742.125)\n",
      "train= 44  loss= variable(77260.44)\n",
      "train= 45  loss= variable(145933.55)\n",
      "train= 46  loss= variable(80078.484)\n",
      "train= 47  loss= variable(74518.94)\n",
      "train= 48  loss= variable(84089.9)\n",
      "train= 49  loss= variable(62727.89)\n",
      "train= 50  loss= variable(85400.4)\n",
      "train= 51  loss= variable(80113.26)\n",
      "train= 52  loss= variable(71073.23)\n",
      "train= 53  loss= variable(81165.484)\n",
      "train= 54  loss= variable(82573.33)\n",
      "train= 55  loss= variable(87552.195)\n",
      "train= 56  loss= variable(67363.37)\n",
      "train= 57  loss= variable(84234.55)\n",
      "train= 58  loss= variable(61339.062)\n",
      "train= 59  loss= variable(107222.5)\n",
      "train= 60  loss= variable(77722.71)\n",
      "train= 61  loss= variable(102492.1)\n",
      "train= 62  loss= variable(88689.08)\n",
      "train= 63  loss= variable(108330.055)\n",
      "train= 64  loss= variable(55370.38)\n",
      "train= 65  loss= variable(84718.9)\n",
      "train= 66  loss= variable(78831.23)\n",
      "train= 67  loss= variable(62111.645)\n",
      "train= 68  loss= variable(110719.6)\n",
      "train= 69  loss= variable(73412.13)\n",
      "train= 70  loss= variable(71528.25)\n",
      "train= 71  loss= variable(82478.89)\n",
      "train= 72  loss= variable(63679.234)\n",
      "train= 73  loss= variable(66463.04)\n",
      "train= 74  loss= variable(53332.516)\n",
      "train= 75  loss= variable(78733.06)\n",
      "train= 76  loss= variable(65524.527)\n",
      "train= 77  loss= variable(74975.164)\n",
      "train= 78  loss= variable(61615.688)\n",
      "train= 79  loss= variable(75774.95)\n",
      "train= 80  loss= variable(83983.74)\n",
      "train= 81  loss= variable(103823.71)\n",
      "train= 82  loss= variable(96807.58)\n",
      "train= 83  loss= variable(75851.88)\n",
      "train= 84  loss= variable(73280.38)\n",
      "train= 85  loss= variable(76293.17)\n",
      "train= 86  loss= variable(62720.19)\n",
      "train= 87  loss= variable(74734.914)\n",
      "train= 88  loss= variable(75718.87)\n",
      "train= 89  loss= variable(99637.125)\n",
      "train= 90  loss= variable(63360.977)\n",
      "train= 91  loss= variable(64874.26)\n",
      "train= 92  loss= variable(72758.41)\n",
      "train= 93  loss= variable(90261.51)\n",
      "train= 94  loss= variable(80160.98)\n",
      "train= 95  loss= variable(68872.72)\n",
      "train= 96  loss= variable(66835.73)\n",
      "train= 97  loss= variable(73581.25)\n",
      "train= 98  loss= variable(65291.723)\n",
      "train= 99  loss= variable(69672.22)\n",
      "train | epoch =3 | time=4663.008904457092 sec | loss=79772.1073828125\n",
      "epoch:   4 / 004, training loss: 79772.1073828125, validation loss: 0.0.\n"
     ]
    }
   ],
   "source": [
    "# Actual of Training\n",
    "import time\n",
    "train_len = 0.79\n",
    "for i in range(epochs):#epochs\n",
    "    epoch = i + 1\n",
    "    loss_history['training'].append(0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        with chainer.using_config('train', True):\n",
    "            t, x = concat_examples(batch=batch, device=device)\n",
    "            y = model(x)\n",
    "            # (1) start\n",
    "            loss = lossFunction(t, y)\n",
    "            # Calculate the gradients in the network\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "            # Update all the trainable parameters\n",
    "            optimizer.update()\n",
    "            # (1) end\n",
    "\n",
    "        loss_history['training'][-1] += float(loss.data)\n",
    "        print('train=',j,' loss=',loss)\n",
    "\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    print('train | epoch ={} | time={} sec | loss={}'.format(i, (time.time() - start_time), \n",
    "                                                             loss_history['training'][-1]) )\n",
    "    \n",
    "    # (2) start\n",
    "    loss_history['validation'].append(0)\n",
    "#     start_time = time.time()\n",
    "#     for j, batch in enumerate(validation_iterator):\n",
    "#         with chainer.using_config('train', False):\n",
    "#             t, x = concat_examples(batch=batch, device=device)\n",
    "#             y = model(x)\n",
    "#             loss = lossFunction(t, y)\n",
    "\n",
    "#         # ...\n",
    "#         loss_history['validation'][-1] += float(loss.data)\n",
    "#         #print('validation',loss)\n",
    "\n",
    "#     loss_history['validation'][-1] /= j + 1\n",
    "#     print('validation | epoch ={} | time={} sec | loss={}'.format(i, (time.time() - start_time), \n",
    "#                                                              loss_history['validation'][-1]) )\n",
    "    loss_history['validation'][-1] = 0.0\n",
    "    # ...\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {}, validation loss: {}.'.format(epoch, epochs, loss_history['training'][-1], loss_history['validation'][-1]))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    # (3) start\n",
    "    serializers.save_npz('{:s}/model_{:03d}.npz'.format(model_directory, epoch), model)\n",
    "    # (3) end\n",
    "    serializers.save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epoch), optimizer)\n",
    "    \n",
    "    training_set = Dataset(data_file[:400])\n",
    "    training_iterator = iterators.SerialIterator(training_set, batch_size, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba85f09198>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F-pOSKTw0tcK"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2f20054e980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Calculate the gradients in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Update all the trainable parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NIPS/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_grad, enable_double_backprop, loss_scale)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \"\"\"\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enable_backprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_double_backprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NIPS/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36m_backward_main\u001b[0;34m(self, retain_grad, loss_scale)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             gxs = func.backward_accumulate(\n\u001b[0;32m-> 1095\u001b[0;31m                 target_input_indexes, out_grad, in_grad)\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NIPS/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mbackward_accumulate\u001b[0;34m(self, target_input_indexes, grad_outputs, grad_inputs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;31m# The default implementation uses backward(). You can override this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# method without using backward().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mgxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_input_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mlen_gxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Questions\n",
    "Learn model training in chainer\n",
    "'''\n",
    "for i in range(epochs):\n",
    "    loss_history['training'].append(0)\n",
    "\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        with chainer.using_config('train', True):\n",
    "            t, x = concat_examples(batch)\n",
    "            y = model(x)\n",
    "            # (1) start\n",
    "            loss = lossFunction(t, y)\n",
    "            # ...\n",
    "            # ...\n",
    "            # ...\n",
    "            # (1) end\n",
    "            # Calculate the gradients in the network\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "            # Update all the trainable parameters\n",
    "            optimizer.update()\n",
    "\n",
    "        loss_history['training'][-1] += float(loss.data)\n",
    "\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    # (2) start\n",
    "    # ...\n",
    "    loss_history['validation'].append(0)\n",
    "\n",
    "    for j, batch in enumerate(validation_iterator):\n",
    "        with chainer.using_config('train', False):\n",
    "            t, x = concat_examples(batch)\n",
    "            y = model(x)\n",
    "            loss = lossFunction(t, y)\n",
    "\n",
    "        # ...\n",
    "        loss_history['validation'][-1] += float(loss.data)\n",
    "\n",
    "    loss_history['validation'][-1] /= j + 1\n",
    "    # ...\n",
    "    \n",
    "    \n",
    "    # (2) end\n",
    "    epoch = i + 1\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {:.4f}, validation loss: {:.4f}.'.format(epoch, epochs, loss_history['training'], loss_history['validation']))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    # (3) start\n",
    "    serializers.save_npz('{:s}/model_{:03d}.npz'.format(model_directory, epoch), model)\n",
    "    # (3) end\n",
    "    serializers.save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epoch), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YivB1PQ7Obh"
   },
   "source": [
    "**Test (45 points + 15 bonus points)**  \n",
    "In the following cell, you will test your model.  \n",
    "*Tasks*\n",
    "- (1) Estimate the test loss, print it and save it. (**15 points**)\n",
    "- (2) Estimate the validation metrics, print them and save them (tip: scikit-image) (**15 bonus points**)\n",
    "- (3) Plot example results (i.e., plot a few t, x and y) (**10 points**)\n",
    "- (4) Dicuss your implementation in 300 - 350 words (e.g., how good your results are, how you can improve your model, etc.) (**20 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdlnCFDS-Cdh"
   },
   "outputs": [],
   "source": [
    "# (1), (2) and (3) start\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# (1), (2) and (3) end\n",
    "\n",
    "# (4) Write your answer here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "weeks_2_and_3_assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
